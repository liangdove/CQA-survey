## 老师给的数据集的评测结果

## 评测思路
由于数据量比较少，且有不少是简答格式的题目（不是判断和选择），所以采用准确率评测指标，评测依据是关键词是否在模型生成内容里（不算层级关系的子集）。

由于不同的prompt会导致不同的生成结果，所以由简到繁设计了三种prompt，并取三次的平均值作为最终模型的准确率。

在之后的ChartQA、MME、MMBench中，分为单词回答（ChartQA）、选择题(MMBench)、判断题(MME)。所以不需要精心设计prompt，标准数据集这样做是为了防止提示工程作弊。

## Prompt:
- test1:请仔细分析这个层级关系图，首先整理理清楚各个模块的折线连接关系、箭头指向、上下关系、左右关系等，然后回答问题：{question}
- test2:请仔细分析这个层级关系图，简要回答问题：{question}
- test3:{question}

## 评测实现
[InternVL3评测代码](../TEST_test_my_own_data/TEST_test_hf.py)
[Ovis2评测代码 (使用了lmdeploy加速推理)](../TEST_test_my_own_data/TEST_test_lmdeploy.py)

## 评测结果

### InternVL3-8B

*括号里的是预测错误的ID*

- 无微调
- 回答准确率：
test1: 9/14（27、267、349、365、498）
test2: 10/14（27、257、349、498）
test3: 11/14（257、349、498）
avg: **0.714**

### Ovis2-8B
*括号里的是预测错误的ID*
- 无微调
- 回答准确率：
test1: 8/14 (27、257、267、349、385、498)
test2: 9/14 (257、267、349、385、498)
test3: 9/14 (257、267、349、385、498)
avg: **0.619**


### InternVL3-14B-4bit
*括号里的是预测错误的ID*
- 无微调
- 回答准确率：
test1: 10/14（349、365、426、498）
test2: 9/14（257、349、385、426、498）
test3: 10/14(257、349、365、498)
avg: **0.690**
